\documentclass[a4paper]{article}
\usepackage[slovene, english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage[T1]{fontenc}
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{Rok Ivanšek}
\rhead{IŠRM, II}
\usepackage[margin=0.8in]{geometry}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsfonts}
\usepackage{hyperref}
\usepackage{color}

\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}

\title{Cryptography and Computer security\\
Classifiying classical cyphers using random forest}
\author{Rok Ivanšek, IŠRM, II}
\date{\today}

% telo dokumenta
\begin{document}



\maketitle


\begin{abstract}
When encrypting a plaintext into a cyphertext, the idea is to hide the actual content of the underlying message, thus making it unreadable for a potential harmful adversary. Cyphertexts are therefore ``gibberish'' texts that do not show any structure that would resemble a natural language. Despite this fact, we can still find patterns in the cyphertexts. The patterns are clearer, when the underlying encryption rule is more transparent, like in the case of classical cyphers. In this report I show that we can use this patterns to match a set of cyphertexts to the corresponding cypher. By carefully engineering features we can use a random forest technique to train a classifier model that is able to classify cyphertexts of classical cyphers.
\end{abstract}

\section*{Introduction and related work}
In my work I followed the typical data sciene pipeline. Getting the data, cleaning it, engineering features, doing some exploratory data analysis, training and tuning the model and in the end interpreting results and extracting information. In this domain classification has been attempted for classical substitution ciphers using a neural network \cite{sivagurunathan2010classification}, modern block cyphers like DES, 3DES and AES using support vector machines \cite{dileep2006identification}. Another aproach was used in \cite{maheshwari2001classification} for both classical and modern cyphers. In \cite{mishra2013pattern} different methods were combined to classify modern block and stream ciphers and a clustering study was done for cyphers from the finalists of the AES contest in 2000 \cite{decipher}.

\section*{Data}

\subsection*{Plaintexts}
For my plaintexts I randomly choose 1000 texts from the 20newsgroups dataset. I used the preprocesed dataset from \href{http://ana.cachopo.org/datasets-for-single-label-text-categorization}{Ana Cardoso Cachopo} \cite{2007:phd-Ana-Cardoso-Cachopo}. The preprocessing made on original texts from the 20newsgroups dataset was:

\begin{itemize}	
	\setlength\itemsep{-0.2em}
    \item substitute TAB, NEWLINE and RETURN characters by SPACE,
    \item keep only letters (that is, turn punctuation, numbers, etc. into SPACES),
    \item turn all letters to lowercase,
    \item substitute multiple SPACES by a single SPACE,
    \item the title/subject of each document is simply added in the beginning of the document's text.
\end{itemize}


In addition to the preprocessing already made, I took out the whitespaces and trimmed the texts to length of 1000 characters. This way I got 1000 strings of length 1000, each representing a part of a randomly selected text from the dataset. This strings were my plaintexts used in the analysis.

\subsection*{Cyphertexts}
To encrypt the plaintexts I used the python library \href{https://github.com/jameslyons/pycipher}{pycipher} \textcolor{red}{TODO include a citation for pycipher here}. To make the classification task as representative as possible, I tried to choose a variety of different cyphers. Including some that would produce simmilar cyphertexts, like the permutation and Caesar cypher and some whose cyphertexts are clearly distinguishable from each other, like the ADFGVX and the Vigenere cypher. While encrypting the key of the cypher was always chosen randomly out of all posible combinations, except where stated differently. I encrypted each plaintext with every chosen cipher. This way I got $6000$ cipherthexts, a $1000$ for each of the chosen ciphers. I ended up choosing the following cyphers.\\
\\
In the description of the cyphers I will always assume that $x = (x_{1}, x_{2}, ...,x_{m})$ is a plaintext consisting of $m$ integers from $\mathbb{Z}_{26}$ which map to letters in the english alphabet.

\subsubsection*{Affine cypher}
The affine cipher is in escence a standard substitusion cypher, meaning that each letter encrypts to one other letter. The key are integers $a$ and $b$. The encryption rule is
$$e(x_{i}) = (ax_{i} + b) \;(\bmod\; 26), \forall i$$
where $a$ is relatively prime to $26$ and $b$ is an arbitrary integer in range $0-25$.

%http://www.practicalcryptography.com/ciphers/affine-cipher/
%https://en.wikipedia.org/wiki/Affine_cipher

\subsubsection*{Vigenere cypher}
The Vigenere cipher originates from the 16th century. It encrypts the ciphertexts in blocks of $n$ characters. For a chosen keyword $K = (k_{1}, k_{2}, ..., k_{n})$ of length $n < m$, it encrypts the plaintext following the rule
$$e(x_{1}, x_{2}, ...,x_{m}) = (x_{1}+k_{1}, x_{2}+k_{2}, ..., x_{n}+k_{n}, x_{n+1}+k_{1}, ...,x_{2n}+k_{n}, x_{2n+1} + k_{1}, ...),$$
where all $+$ operations are done $(\bmod\; 26)$.

\subsubsection*{ADFGVX cipher}
The ADFGVX was a field cyphter used by the German Army during World War 1. It uses a modified Polybius square and a single columnar transposition to encrypt the plaintext. The complete procedure of encryption is too long to describe in this report. The important properties of this cipher are that it uses only the letters A,D,F,G,V and X in the ciphertexts and the ciphertext is two times as long as the plaintext. 

\subsubsection*{Caesar cipher}
The Caesar cipher is one of the earliest known and simplest ciphers. It is a shift cipher that shifts all the letters in the plaintexts by a key value $k$. It can be viewed a special case of the Vigenere cipher, where the key is of legth $1$. The encryption rule is
$$e(x_{i}) = (x + k) \;(\bmod\; 26), \forall i$$

\subsubsection*{Permutation cipher}
Also called the simple substitution cipher, it has been in use for many hundreds of years. It basically consists of substituting every plaintext character for a different ciphertext character. It differs from the Caesar cipher in that the cipher alphabet is not simply the alphabet shifted, it is completely jumbled. The key for this cipher is simply a jumbled alphabet. For example a key $K = AJPCZWRLFBDKOTYUQGENHXMIVS$ would impose the following encryption rule
$$ABCDEFGHIJKLMNOPQRSTUVWXYZ \rightarrow AJPCZWRLFBDKOTYUQGENHXMIVS,$$
meaning a letter in the left string would encrypt into a letter in the right string that is in the same place. For example $M$ would encrypt to $T$.

\subsubsection*{Playfair cipher}
The Playfair cipher is a digraph substitution cipher from the 19th century that was also used in the World War 1 by the British forces. It encrypts pairs (digraphs) of letters using a key square. Again the full description of the encryption is too long to include in this report.\\
\\
\textcolor{red}{TODO: Add descriptions if necesary.}


\section*{Feature engineering}
%The first idea was to just use letters in a ciphertext as features (the first letter is the first feature, the second letter the second feature, etc...) and than use deep learning (a neural network with a lot of hiddent layers) as the model. Hopefully the neural network would discover some interesting patterns and correlations in the data, that would allow it to distinguish beetween different ciphertexts. Setting up such a network is not an easy task and there is no guarantee that this method will work either, since it has not been tried in this domain yet to my knowledge. So after some initial runs of the neural network and getting really bad results I focused on engeniring useful features that would make the job easier for the classfier. \\
In order to express patterns in the cyphertexts I had to come up with interesting features. A feature is in essence any property of the cyphertexts that can be expressed in a qualitative or a quanititative way. The features that I decided to use are closely related to the properties of the cyphers I choose. They are mostly quantitative measurements that are used, when one attempts to break the chosen cyphers.

\subsubsection*{Distribution of letters}
The first most simple group of features one can extract is the distribution of letters. This just means that we count, how many times each letter appears in the cyphertext. This way we obtain $26$ features, one for each letter in the alphatbet. Using only this simple froup of features we would allready be able to distinguish between some of the ciphers that use only a subset of letters from the alphabet, like the ADFGVX from other ciphers that use the whole alphabet.

\begin{figure}[H]
    \centering
    \begin{subfigure}[h]{0.5\textwidth}
    		\centering
        \includegraphics[height=2.5in]{img/affine_dist.png}
    \end{subfigure}%
    ~
    \begin{subfigure}[h]{0.5\textwidth}
	    \centering
        \includegraphics[height=2.5in]{img/adfgvx_dist.png}
    \end{subfigure}
    \caption{Average distribution of letters in the cyphertexts for the Affine cipher (left) and the ADFGVX cipher (right).}
    %\cite{g2:NPMP_prosojnice_mraz_biomodeliranje}}
    \label{fig:letter_dist}
\end{figure}

\subsubsection*{Adjacent duplicates}
Another simple feature that proved to be efficient in a simmilar cyphertexts classification task \cite{sivagurunathan2010classification} is the number of adjacent duplicates. It is particulary usefull when recognizing the Playfair cipher, since this cipher should have a substantially lower number of adjacent duplicates than other ciphers, due to its encryption rule.

\begin{figure}[H]
    \centering
    \begin{subfigure}[h]{0.5\textwidth}
        \centering
        \includegraphics[height=2.5in]{img/no_adj_dups_log.png}
    \end{subfigure}
    \caption{Logarithm of the average number of adjacent duplicates for the used ciphers.}
    \label{fig:adj_dups}
\end{figure}

\subsubsection*{Repeating bigrams}
In a classical text classfication an $n$-gram (usually a bigram) is a sequence of $n$ adjacent tokens in a text. Tokens can be words, syllables or letters. Using bigrams of letters I extracted two features. First was the number of unique bigrams that repeat at least once in a ciphertext and second was the frequency of the most frequent bigram in a cyphertext.

\begin{figure}[H]
    \centering
    \begin{subfigure}[h]{0.5\textwidth}
    		\centering
        \includegraphics[height=2.5in]{img/bigrams_1.png}
    \end{subfigure}%
    ~
    \begin{subfigure}[h]{0.5\textwidth}
	    \centering
        \includegraphics[height=2.5in]{img/bigrams_2.png}
    \end{subfigure}
    \caption{Average number of unique repeating bigrams (left) and the average frequency of the most appearing bigram (right) in the ciphertexts.}
    %\cite{g2:NPMP_prosojnice_mraz_biomodeliranje}}
    \label{fig:bigrams}
\end{figure}

\subsubsection*{Index of coincidence}
Index of coincidence (IC) along with Kasiski's test is used in breaking the Vigenere cypher. It tells us the probability that we will get two matching letters, if we randomly select two letters from a given text. The formula for calculating the IC for a text $x$ is
$$IC(x) = \sum_{i=0}^{25} \frac{f_{i}(f_{i}-1)}{d(d-1)}, $$
where $f_{i}$ are the frequencies of letters in $x$ and $d$ is the length of $x$.

\begin{figure}[H]
    \centering
    \begin{subfigure}[h]{0.5\textwidth}
        \centering
        \includegraphics[height=2.5in]{img/iocs.png}
    \end{subfigure}
    \caption{Average index of coincidence.}
    \label{fig:ioc}
\end{figure}

The IC of an English plaintext is approximately $0.065$. The Affine, Caesar and Permutation ciphers are examples of single letter substitution ciphers, where the mapping that determines the encrypion of letters is bijective (one-to-one). It is therefore no surprise that the IC of their ciphertexts is the same as the IC of the English plaintexts.

\section*{Exploratory data analysis}
\textcolor{red}{TODO: Include some graphs to show if chosen features are good and can be used to distinguish between the classes. Do a clustering study. Show scatterplots, bargraphs. Consider ploting variable corelations. https://www.mathworks.com/help/econ/corrplot.html}\\
\\
One common step before moving on to actual training of the model, is to draw some rough graphs representing the data that will be used for training. This is to see how features interact with each other and to get an idea of how the data clusters.\\
\\
Here I present just one graph to show that the chosen features separate the data in a clear way. I use a subsample of $10\%$ of the data. The features were scaled zero mean and unit variance, and then multidimensional scalling was used to shrink down the data to two dimensions.

\begin{figure}[H]
    \centering
    \begin{subfigure}[h]{0.5\textwidth}
        \centering
        \includegraphics[height=2.5in]{img/mds_features.png}
    \end{subfigure}
    \caption{MDS(multidimensional scaling) used on a subsample of the data.}
    \label{fig:mds}
\end{figure}

From graph \ref{fig:mds} we can see how the ADFGVX cipher is clearly separated from the others. Vigenere and Playifair appearing in the inner circles are also separable from the other ciphers, while the Affine, Ceasar and Permutation cipher all lay mixed together in the outher most circle. This is an encouraging sight. It tells us that the classifier should produce good results.

\section*{Random forest classifier}
Random forest is an ensemble method. It works by building multiple simple desicion trees on different subsamples of the data and than combining the results produce a stronger model. It can be used for classification or regression. I will not go into detail about the workings of the descision tree and random forest techniques, since this is a broad topic and it is not the main focus of this report. I advise the reader to learn more in \cite{breiman2001random}. For now it suffices to say that this is a robust machine learning technique that works well on different domains in data science. The main parameter of the random forest is the number of decision trees built. It generally holds that more is better and we usually just build as much trees as our computing capabilities allow us, but the accuracy of the model is expected to stop improving substantially at some point.

\begin{figure}[H]
    \centering
    \begin{subfigure}[h]{0.5\textwidth}
        \centering
        \includegraphics[height=2.5in]{img/accuracy_rf.png}
    \end{subfigure}
    \caption{Determining the optimal number of decision trees in the random forest.}
    \label{fig:acc_rf}
\end{figure}

On graph \ref{fig:acc_rf} we can see how the accuracy score of the random forest changes as we increase the number of descision trees in the forest. We can see that the accuracy of the model does not change significantly for more that $2000$ decision trees. The score was calculated on the training set using a $5$-fold cross validation. 

\section*{Results}
\textcolor{red}{TODO: Present the prediction results (accuracy) in different ways. Using different metrics (total accuracy, tp, fp, tn, fn, ...)...}
Training a random forest on the train portion of the data, building $2000$ decision trees, I obtained an accuracy of $0.9158$.
\textcolor{red}{TODO: Show sklearn classification report in a table. Explain what the metrics mean. Comment on results. Also determine misclassification of Affine, Ceasar and Permutation cyphers and show this in a table.}

\section*{Comments}
\textcolor{red}{TODO: Comment on the prediction results. Is your method a good one?}
\textcolor{red}{TODO: Cite sklearn, Stinsons book, practical cryptography website, matplotlib, pycipher} library.

\bibliographystyle{plain}
\bibliography{sample}

\end{document} 