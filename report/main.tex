\documentclass[a4paper]{article}
\usepackage[slovene]{babel}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage[T1]{fontenc}
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{Rok Ivanšek}
\rhead{IŠRM, II}
\usepackage[margin=0.8in]{geometry}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsfonts}
\usepackage{hyperref}


\title{Cryptography and Computer security\\
Classifiying classical cyphers using neural networks}
\author{Rok Ivanšek, IŠRM, II}
\date{\today}

% telo dokumenta
\begin{document}



\maketitle


\begin{abstract}
TODO: Write a short abstract
\end{abstract}

\section*{Data}

\subsection*{Plaintexts}
TODO: How did you generate the data (plaitexts)
I randomly choose 1000 texts from the 20newsgroups dataset. I used the preprocesed dataset from \href{http://ana.cachopo.org/datasets-for-single-label-text-categorization}{Ana Cardoso Cachopo} \cite{2007:phd-Ana-Cardoso-Cachopo}. The preprocessing made on original texts from the 20newsgroups dataset were:

\begin{itemize}	
    \item substitute TAB, NEWLINE and RETURN characters by SPACE,
    \item keep only letters (that is, turn punctuation, numbers, etc. into SPACES),
    \item turn all letters to lowercase,
    \item substitute multiple SPACES by a single SPACE,
    \item the title/subject of each document is simply added in the beginning of the document's text.
\end{itemize}

In addition to the preprocessing already made, I took out the whitespaces and trimmed the texts to length of 1000 characters. This way I got 1000 strings of length 1000, each representing a part of a randomly selected text from the dataset. This strings were my chosen plaintexts for analysis.

\subsection*{Cyphertexts}
TODO: How did you encrypt the plaintexts.
To encrypt the plaintexts I used the python library \href{https://github.com/jameslyons/pycipher}{pycipher}.

For this analysis I chose the following encryption algorithms:

\subsection{*Vigenere cypher}
\TODO: Short description of the Vigenere cypher.

The Vigenere cypher has the following properties:

\begin{itemize}
	\item 
\end{itemize}

\subsection{*Affine cypher}
\TODO: Short description of the Affine cypher.

The Affine cypher has the following properties:

\begin{itemize}
	\item 
\end{itemize}


\begin{itemize}
	\item Ceasar cypher
	\item Vigenere cypher
	\item Affine cypher
	\item itd.
\end{itemize}

I encrypted each plaintext with every encryption algorithm. This way I got n000 cypherthexts, 1000 for each of the choseng encrytion algorithms.

\section*{Feature engineering}
The first idea was to just use letters in a cyphertext as features (the first letter is the first feature, the second letter the second feature, etc...) and than use deep learning (a neural network with a lot of hiddent layers) as the model. Hopefully the neural network would discover some interesting patterns and correlations in the data, that would allow it to distinguish beetween different cyphertexts. Setting up such a network is not an easy task and there is no guarantee that this method will work either, since it has not been tried in this domain yet to my knowledge. So after some initial runs of the neural network and getting really bad results I focused on engeniring useful features that would make the job easier for the classfier. \\
Taking into account the properties of the encryption algorithms used I extracted the following features from the cyphertexts:

\begin{itemize}
	\item TODO: insert usefull feature here :)
\end{itemize}

\section*{ML techniques used}
TODO: Describe the architecture of the NN, the activation function you used. Maybe you will do a one vs one classification i.e. for each class you train an individual NN classifier. You feed the new sample into each NN and than join results (use the normalization from OZIP).


\section*{Results}
TODO: Present the prediction results (accuracy) in different ways. Using different metrics (total accuracy, tp, fp, tn, fn, ...)...


\section*{Comments}
TODO: Comment on the prediction results. Is your method a good one?

\bibliographystyle{plain}
\bibliography{sample}

\end{document} 